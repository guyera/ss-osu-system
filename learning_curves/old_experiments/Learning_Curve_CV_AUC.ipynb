{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62240ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-1-2008f453f384>\u001b[0m(283)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    281 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    282 \u001b[0;31m        \u001b[0maucs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 283 \u001b[0;31m            adaptation.supervised_anomaly_detectors.train_supervised_models_nom_anom(\n",
      "\u001b[0m\u001b[0;32m    284 \u001b[0;31m                \u001b[0mS_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mO_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    285 \u001b[0;31m                \u001b[0mS_unsupervised_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV_unsupervised_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mO_unsupervised_scores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> len(S_examples)\n",
      "10\n",
      "ipdb> c\n",
      "\n",
      "Round 0 Complete!\n",
      "Subject Partial AUC is 0.4285714285714286\n",
      "This is -0.31279987184108293 better than unsupervised\n",
      "Verb Partial AUC is 0.4285714285714286\n",
      "This is -0.11904229610467032 better than unsupervised\n",
      "Object Partial AUC is 0.4285714285714286\n",
      "This is -0.3787306357243564 better than unsupervised\n",
      "Subject AUC is 0.10000000000000003\n",
      "This is -0.7703847771236334 better than unsupervised\n",
      "Verb AUC is 1.0\n",
      "This is 0.35069677323198445 better than unsupervised\n",
      "Object AUC is 1.0\n",
      "This is 0.10198976888803979 better than unsupervised\n",
      "> \u001b[0;32m<ipython-input-1-2008f453f384>\u001b[0m(280)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    278 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    279 \u001b[0;31m        \u001b[0;31m### BREAK HERE AND CHECK {}_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 280 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    281 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    282 \u001b[0;31m        \u001b[0maucs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> len(S_examples)\n",
      "20\n",
      "ipdb> len(S_examples)\n",
      "20\n",
      "ipdb> n\n",
      "> \u001b[0;32m<ipython-input-1-2008f453f384>\u001b[0m(283)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    281 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    282 \u001b[0;31m        \u001b[0maucs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 283 \u001b[0;31m            adaptation.supervised_anomaly_detectors.train_supervised_models_nom_anom(\n",
      "\u001b[0m\u001b[0;32m    284 \u001b[0;31m                \u001b[0mS_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mO_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    285 \u001b[0;31m                \u001b[0mS_unsupervised_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV_unsupervised_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mO_unsupervised_scores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> c\n",
      "\n",
      "Round 1 Complete!\n",
      "Subject Partial AUC is 0.4285714285714286\n",
      "This is -0.31279987184108293 better than unsupervised\n",
      "Verb Partial AUC is 0.4285714285714286\n",
      "This is -0.11904229610467032 better than unsupervised\n",
      "Object Partial AUC is 0.4285714285714286\n",
      "This is -0.3787306357243564 better than unsupervised\n",
      "Subject AUC is 0.5499999999999999\n",
      "This is -0.32038477712363345 better than unsupervised\n",
      "Verb AUC is 0.9999999999999999\n",
      "This is 0.35069677323198434 better than unsupervised\n",
      "Object AUC is 0.9999999999999999\n",
      "This is 0.10198976888803968 better than unsupervised\n",
      "> \u001b[0;32m<ipython-input-1-2008f453f384>\u001b[0m(280)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    278 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    279 \u001b[0;31m        \u001b[0;31m### BREAK HERE AND CHECK {}_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 280 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    281 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    282 \u001b[0;31m        \u001b[0maucs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> S_examples\n",
      "tensor([[ 0.1378, -0.0461, -0.2976,  ..., -0.5043, -1.4665, -1.6844],\n",
      "        [-0.1148, -1.2183, -1.4835,  ..., -0.7896, -1.0903, -0.8029],\n",
      "        [ 0.0099,  0.0134, -1.0669,  ...,  1.5476, -0.0143, -0.6062],\n",
      "        ...,\n",
      "        [ 0.2246, -0.3762, -0.8816,  ..., -1.1421, -1.5383, -2.1952],\n",
      "        [ 0.0985, -0.0210,  0.0642,  ..., -0.3804, -0.1897, -1.3327],\n",
      "        [-0.3431, -0.0896, -0.3070,  ..., -0.2252, -0.6397, -1.1330]],\n",
      "       device='cuda:0')\n",
      "ipdb> len(S_examples)\n",
      "30\n",
      "ipdb> S_labels\n",
      "tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]], device='cuda:0')\n",
      "ipdb> n\n",
      "> \u001b[0;32m<ipython-input-1-2008f453f384>\u001b[0m(283)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    281 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    282 \u001b[0;31m        \u001b[0maucs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 283 \u001b[0;31m            adaptation.supervised_anomaly_detectors.train_supervised_models_nom_anom(\n",
      "\u001b[0m\u001b[0;32m    284 \u001b[0;31m                \u001b[0mS_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mO_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    285 \u001b[0;31m                \u001b[0mS_unsupervised_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV_unsupervised_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mO_unsupervised_scores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> n\n",
      "> \u001b[0;32m<ipython-input-1-2008f453f384>\u001b[0m(284)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    282 \u001b[0;31m        \u001b[0maucs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    283 \u001b[0;31m            adaptation.supervised_anomaly_detectors.train_supervised_models_nom_anom(\n",
      "\u001b[0m\u001b[0;32m--> 284 \u001b[0;31m                \u001b[0mS_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mO_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    285 \u001b[0;31m                \u001b[0mS_unsupervised_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV_unsupervised_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mO_unsupervised_scores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    286 \u001b[0;31m                \u001b[0mS_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mO_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> n\n",
      "> \u001b[0;32m<ipython-input-1-2008f453f384>\u001b[0m(285)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    283 \u001b[0;31m            adaptation.supervised_anomaly_detectors.train_supervised_models_nom_anom(\n",
      "\u001b[0m\u001b[0;32m    284 \u001b[0;31m                \u001b[0mS_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mO_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 285 \u001b[0;31m                \u001b[0mS_unsupervised_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV_unsupervised_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mO_unsupervised_scores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    286 \u001b[0;31m                \u001b[0mS_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mO_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    287 \u001b[0;31m            )\n",
      "\u001b[0m\n",
      "ipdb> n\n",
      "> \u001b[0;32m<ipython-input-1-2008f453f384>\u001b[0m(286)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    284 \u001b[0;31m                \u001b[0mS_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mO_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    285 \u001b[0;31m                \u001b[0mS_unsupervised_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV_unsupervised_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mO_unsupervised_scores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 286 \u001b[0;31m                \u001b[0mS_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mO_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    287 \u001b[0;31m            )\n",
      "\u001b[0m\u001b[0;32m    288 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> n\n",
      "> \u001b[0;32m<ipython-input-1-2008f453f384>\u001b[0m(289)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    287 \u001b[0;31m            )\n",
      "\u001b[0m\u001b[0;32m    288 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 289 \u001b[0;31m        \u001b[0mS_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mO_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    290 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    291 \u001b[0;31m        \u001b[0mS_nom_scores\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mS_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipdb> n\n",
      "> \u001b[0;32m<ipython-input-1-2008f453f384>\u001b[0m(291)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    289 \u001b[0;31m        \u001b[0mS_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mO_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    290 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 291 \u001b[0;31m        \u001b[0mS_nom_scores\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mS_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    292 \u001b[0;31m        \u001b[0mS_nom_trues\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS_nom_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    293 \u001b[0;31m        \u001b[0mS_anom_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mS_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> n\n",
      "> \u001b[0;32m<ipython-input-1-2008f453f384>\u001b[0m(292)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    290 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    291 \u001b[0;31m        \u001b[0mS_nom_scores\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mS_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 292 \u001b[0;31m        \u001b[0mS_nom_trues\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS_nom_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    293 \u001b[0;31m        \u001b[0mS_anom_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mS_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    294 \u001b[0;31m        \u001b[0mS_anom_trues\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS_anom_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> n\n",
      "> \u001b[0;32m<ipython-input-1-2008f453f384>\u001b[0m(293)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    291 \u001b[0;31m        \u001b[0mS_nom_scores\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mS_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    292 \u001b[0;31m        \u001b[0mS_nom_trues\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS_nom_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 293 \u001b[0;31m        \u001b[0mS_anom_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mS_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    294 \u001b[0;31m        \u001b[0mS_anom_trues\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS_anom_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    295 \u001b[0;31m        \u001b[0msubj_trues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS_nom_trues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS_anom_trues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> n\n",
      "> \u001b[0;32m<ipython-input-1-2008f453f384>\u001b[0m(294)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    292 \u001b[0;31m        \u001b[0mS_nom_trues\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS_nom_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    293 \u001b[0;31m        \u001b[0mS_anom_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mS_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 294 \u001b[0;31m        \u001b[0mS_anom_trues\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS_anom_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    295 \u001b[0;31m        \u001b[0msubj_trues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS_nom_trues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS_anom_trues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    296 \u001b[0;31m        \u001b[0mS_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS_nom_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS_anom_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> n\n",
      "> \u001b[0;32m<ipython-input-1-2008f453f384>\u001b[0m(295)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    293 \u001b[0;31m        \u001b[0mS_anom_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mS_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    294 \u001b[0;31m        \u001b[0mS_anom_trues\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS_anom_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 295 \u001b[0;31m        \u001b[0msubj_trues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS_nom_trues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS_anom_trues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    296 \u001b[0;31m        \u001b[0mS_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS_nom_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS_anom_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    297 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> compute_partial_auc(S_anom_scores, S_nom_scores)\n",
      "*** NameError: name 'compute_partial_auc' is not defined\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import adaptation\n",
    "import noveltydetection\n",
    "import noveltydetectionfeatures\n",
    "import unsupervisednoveltydetection\n",
    "import pickle\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from auc import compute_auc\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "examples_per_trial = 10\n",
    "\n",
    "dataset = noveltydetectionfeatures.NoveltyFeatureDataset(\n",
    "        name = 'Custom',\n",
    "        data_root = 'Custom',\n",
    "        csv_path = 'Custom/csvs/dataset_v3_4_val.csv',\n",
    "        #num_subj_cls = 5,\n",
    "        #num_obj_cls = 12,\n",
    "        #num_action_cls = 8,\n",
    "        training = False,\n",
    "        image_batch_size = 16,\n",
    "        feature_extraction_device = 'cuda:0'\n",
    "    )\n",
    "\n",
    "spat_f = dataset.__dict__['spatial_features']\n",
    "s_f = dataset.__dict__['subject_appearance_features']\n",
    "v_f = dataset.__dict__['verb_appearance_features']\n",
    "o_f = dataset.__dict__['object_appearance_features']\n",
    "\n",
    "mask = [i for i in range(len(spat_f)) if (s_f[i] != None and v_f[i] != None and o_f[i] != None)]\n",
    "\n",
    "dataset.__dict__['spatial_features'] = [dataset.__dict__['spatial_features'][i] for i in mask]  \n",
    "dataset.__dict__['subject_labels'] = [dataset.__dict__['subject_labels'][i] for i in mask] \n",
    "dataset.__dict__['object_labels'] = [dataset.__dict__['object_labels'][i] for i in mask] \n",
    "dataset.__dict__['verb_labels'] = [dataset.__dict__['verb_labels'][i] for i in mask] \n",
    "dataset.__dict__['subject_appearance_features'] = [dataset.__dict__['subject_appearance_features'][i] for i in mask] \n",
    "dataset.__dict__['object_appearance_features'] = [dataset.__dict__['object_appearance_features'][i] for i in mask] \n",
    "dataset.__dict__['verb_appearance_features'] = [dataset.__dict__['verb_appearance_features'][i] for i in mask] \n",
    "\n",
    "ci_confidence = 0.99\n",
    "num_splits = 1 #5\n",
    "meta_partial_subj_aucs = []\n",
    "meta_partial_verb_aucs = []\n",
    "meta_partial_obj_aucs  = []\n",
    "meta_subj_aucs = []\n",
    "meta_verb_aucs = []\n",
    "meta_obj_aucs  = []\n",
    "\n",
    "meta_meta_subj_trues = []\n",
    "meta_meta_verb_trues = []\n",
    "meta_meta_obj_trues  = []\n",
    "meta_meta_subj_scores = []\n",
    "meta_meta_verb_scores = []\n",
    "meta_meta_obj_scores  = []\n",
    "for i in range(1,num_splits+1):\n",
    "    with open('Custom/us_s_learning_curves/results{}.pkl'.format(i), 'rb') as f:\n",
    "        unsupervised_results = pickle.load(f)\n",
    "\n",
    "    classifier_state_dict = unsupervised_results['classifier']\n",
    "\n",
    "    uns_subject_auc = unsupervised_results['subject_auc']\n",
    "    uns_object_auc = unsupervised_results['object_auc']\n",
    "    uns_verb_auc = unsupervised_results['verb_auc']\n",
    "\n",
    "    uns_partial_subject_auc = unsupervised_results['partial_subject_auc']\n",
    "    uns_partial_object_auc = unsupervised_results['partial_object_auc']\n",
    "    uns_partial_verb_auc = unsupervised_results['partial_verb_auc']\n",
    "\n",
    "    id_subject_labels = unsupervised_results['id_subject_labels']\n",
    "    id_object_labels = unsupervised_results['id_object_labels']\n",
    "    id_verb_labels = unsupervised_results['id_verb_labels']\n",
    "\n",
    "    ood_subject_labels = unsupervised_results['ood_subject_labels']\n",
    "    ood_object_labels = unsupervised_results['ood_object_labels']\n",
    "    ood_verb_labels = unsupervised_results['ood_verb_labels']\n",
    "\n",
    "    classifier = unsupervisednoveltydetection.common.Classifier(\n",
    "        12544,\n",
    "        12616,\n",
    "        1024,\n",
    "        len(id_subject_labels) + 1, # Add 1 for anomaly label = 0\n",
    "        len(id_object_labels) + 1, # Add 1 for anomaly label = 0\n",
    "        len(id_verb_labels) + 1 # Add 1 for anomaly label = 0\n",
    "    )\n",
    "\n",
    "    classifier = classifier.to(device)\n",
    "\n",
    "    classifier.load_state_dict(classifier_state_dict)\n",
    "\n",
    "    # NOTE: All feature data are lists of torch tensors \n",
    "    # currently.\n",
    "\n",
    "    A_s = ood_subject_labels\n",
    "    A_v = ood_verb_labels\n",
    "    A_o = ood_object_labels\n",
    "\n",
    "    s_class_labels = list(dataset.__dict__['subject_labels'])\n",
    "    v_class_labels = list(dataset.__dict__['verb_labels'])\n",
    "    o_class_labels = list(dataset.__dict__['object_labels'])\n",
    "\n",
    "    # 0 indicates anomaly status\n",
    "    S_y = torch.tensor([0 if s_class_labels[i] in A_s else 1 for i in range(len(s_class_labels))])\n",
    "    S_y = torch.atleast_2d(S_y).T\n",
    "\n",
    "    V_y = torch.tensor([0 if v_class_labels[i] in A_v else 1 for i in range(len(v_class_labels))])\n",
    "    V_y = torch.atleast_2d(V_y).T\n",
    "\n",
    "    O_y = torch.tensor([0 if o_class_labels[i] in A_o else 1 for i in range(len(o_class_labels))])\n",
    "    O_y = torch.atleast_2d(O_y).T\n",
    "\n",
    "    # Making X_features into a tensor instead of a list \n",
    "    # of tensors\n",
    "\n",
    "    S_features = dataset.__dict__['subject_appearance_features']\n",
    "\n",
    "    S_features = torch.stack(S_features)\n",
    "    S_features = torch.squeeze(S_features)\n",
    "\n",
    "    S_X = torch.flatten(S_features, start_dim=1, end_dim=3)   \n",
    "\n",
    "    V_features = dataset.__dict__['verb_appearance_features']\n",
    "    V_features = torch.stack(V_features)\n",
    "    V_features = torch.squeeze(V_features)\n",
    "\n",
    "    V_X = torch.flatten(V_features, start_dim=1, end_dim=3)   \n",
    "\n",
    "    O_features = dataset.__dict__['object_appearance_features']\n",
    "    O_features = torch.stack(O_features)\n",
    "    O_features = torch.squeeze(O_features)\n",
    "\n",
    "    O_X = torch.flatten(O_features, start_dim=1, end_dim=3)   \n",
    "\n",
    "    # Continue to run loop over dataset, stitching\n",
    "    # together pieces as specified by Alex in his\n",
    "    # email. This should produce anomaly scores on an\n",
    "    # element by element basis that can then be used\n",
    "    # to help train the supervised anomaly detectors.\n",
    "\n",
    "    S_a = []\n",
    "    V_a = []\n",
    "    O_a = []\n",
    "\n",
    "    for example_spatial_features, \\\n",
    "        example_subject_appearance_features, \\\n",
    "        example_object_appearance_features, \\\n",
    "        example_verb_appearance_features, \\\n",
    "        _, _, _ in dataset:\n",
    "\n",
    "        verb_features = torch.flatten(example_verb_appearance_features)\n",
    "        spat_features = torch.flatten(example_spatial_features)\n",
    "\n",
    "        subj_clf_features = torch.flatten(example_subject_appearance_features)\n",
    "        obj_clf_features  = torch.flatten(example_object_appearance_features)\n",
    "        verb_clf_features = torch.cat((spat_features, verb_features))\n",
    "\n",
    "        # Unsqueeze along dimension 0 to generate batch tensors\n",
    "        # of length 1\n",
    "        subj_clf_features = torch.unsqueeze(subj_clf_features, 0)\n",
    "        obj_clf_features  = torch.unsqueeze(obj_clf_features,  0)\n",
    "        verb_clf_features = torch.unsqueeze(verb_clf_features, 0)\n",
    "\n",
    "        S_batch_scores = classifier.score_subject(subj_clf_features.to(device))\n",
    "        V_batch_scores = classifier.score_verb(verb_clf_features.to(device))\n",
    "        O_batch_scores = classifier.score_object(obj_clf_features.to(device))\n",
    "\n",
    "        # Assert batch size is 1, an assumption we're making here\n",
    "        assert(S_batch_scores.shape[0] == 1)\n",
    "        S_a.append(torch.squeeze(S_batch_scores, 0))\n",
    "\n",
    "        assert(V_batch_scores.shape[0] == 1)\n",
    "        V_a.append(torch.squeeze(V_batch_scores, 0))\n",
    "\n",
    "        assert(O_batch_scores.shape[0] == 1)\n",
    "        O_a.append(torch.squeeze(O_batch_scores, 0))\n",
    "\n",
    "    S_a = torch.atleast_2d(torch.stack(S_a)).T\n",
    "    V_a = torch.atleast_2d(torch.stack(V_a)).T\n",
    "    O_a = torch.atleast_2d(torch.stack(O_a)).T\n",
    "\n",
    "    # BREAK HERE FOR BAR CHARTS\n",
    "\n",
    "    # Need to utilize max anomaly score querying strategy\n",
    "    # for feedback selection in trials. Could do this by\n",
    "    # sorting the anomaly scores for each appearance \n",
    "    # feature type and imposing the sorted order on \n",
    "    # the appearance features and the labels as well.\n",
    "    #\n",
    "    # This would effectively require 3 orderings:\n",
    "    #    i) Subject Feature Ordering\n",
    "    #   ii) Verb Feature Ordering\n",
    "    #  iii) Object Feature Ordering\n",
    "\n",
    "    subj_vals, subj_idxs = torch.sort(S_a, dim=0, descending=True)\n",
    "    verb_vals, verb_idxs = torch.sort(V_a, dim=0, descending=True)\n",
    "    obj_vals, obj_idxs = torch.sort(O_a, dim=0, descending=True)\n",
    "\n",
    "    subj_idxs = torch.squeeze(subj_idxs)\n",
    "    verb_idxs = torch.squeeze(verb_idxs)\n",
    "    obj_idxs  = torch.squeeze(obj_idxs)\n",
    "\n",
    "    # Sort {}_a {}_X and {}_y according to {}_idxs order.\n",
    "    # This imposes our query selection ordering, so now\n",
    "    # we can simply run batches through in this order\n",
    "    # to simulate.\n",
    "    #\n",
    "    # NOTE: This approach will provide a lower bound on when the \n",
    "    # supervised detector will supercede the unsupervised detector\n",
    "    # since we are ranking according to anomaly scores over the \n",
    "    # WHOLE validation set and then training in batches accordingly.\n",
    "    S_a = S_a[subj_idxs]\n",
    "    S_X = S_X[subj_idxs]\n",
    "    S_y = S_y[subj_idxs]\n",
    "\n",
    "    V_a = V_a[verb_idxs]\n",
    "    V_X = V_X[verb_idxs]\n",
    "    V_y = V_y[verb_idxs]\n",
    "\n",
    "    O_a = O_a[obj_idxs]\n",
    "    O_X = O_X[obj_idxs]\n",
    "    O_y = O_y[obj_idxs]\n",
    "\n",
    "    # We have floor(len(train dataset) / examples_per_trial) rounds here\n",
    "    # start with examples_per_trial examples being sent to the super-\n",
    "    # vised anomaly detectors for training\n",
    "\n",
    "    # ISSUE: If we rely on the mean_AUC returned during\n",
    "    # training, this will be uninformative for small batch\n",
    "    # sizes because each ensemble member will only be trained\n",
    "    # on 1 or two examples. Potential solution, could ignore \n",
    "    # them and hold out half of data for eval during each round.\n",
    "\n",
    "    # Note: Trials are data-cumulative. i.e. If we are\n",
    "    # on round 5, we are utilizing feedback data introduced\n",
    "    # in round 5 as well as the feedback data provided in \n",
    "    # rounds 4,3,2,1,0.\n",
    "\n",
    "    subj_aucs   = []\n",
    "    subj_partial_aucs = []\n",
    "    verb_aucs   = []\n",
    "    verb_partial_aucs = []\n",
    "    object_aucs = []\n",
    "    object_partial_aucs = []\n",
    "    \n",
    "    meta_subj_trues = []\n",
    "    meta_verb_trues = []\n",
    "    meta_obj_trues  = []\n",
    "    meta_subj_scores = []\n",
    "    meta_verb_scores = []\n",
    "    meta_obj_scores  = []\n",
    "    \n",
    "    subj_cis   = []\n",
    "    verb_cis  = []\n",
    "    obj_cis = []\n",
    "    \n",
    "    num_rounds = S_X.shape[0] // examples_per_trial\n",
    "\n",
    "    ### Let the rounds commence!\n",
    "    for j in range(num_rounds):\n",
    "        round_idxs = [i for i in range((j+1)*examples_per_trial)]\n",
    "\n",
    "        S_examples      = S_X[round_idxs].to(device)\n",
    "        S_labels              = S_y[round_idxs].to(device)\n",
    "        S_unsupervised_scores = S_a[round_idxs].to(device)\n",
    "\n",
    "        V_examples      = V_X[round_idxs].to(device)\n",
    "        V_labels              = V_y[round_idxs].to(device)\n",
    "        V_unsupervised_scores = V_a[round_idxs].to(device)\n",
    "\n",
    "        O_examples      = O_X[round_idxs].to(device)\n",
    "        O_labels              = O_y[round_idxs].to(device)\n",
    "        O_unsupervised_scores = O_a[round_idxs].to(device)\n",
    "\n",
    "        ### BREAK HERE AND CHECK {}_labels\n",
    "        import pdb; pdb.set_trace()\n",
    "        \n",
    "        aucs, scores, models = \\\n",
    "            adaptation.supervised_anomaly_detectors.train_supervised_models_nom_anom(\n",
    "                S_examples, V_examples, O_examples,\n",
    "                S_unsupervised_scores, V_unsupervised_scores, O_unsupervised_scores,\n",
    "                S_labels, V_labels, O_labels\n",
    "            )\n",
    "        \n",
    "        S_scores, V_scores, O_scores = scores\n",
    "        \n",
    "        S_nom_scores  = S_scores[0]\n",
    "        S_nom_trues   = torch.ones_like(S_nom_scores)\n",
    "        S_anom_scores = S_scores[1]\n",
    "        S_anom_trues  = torch.zeros_like(S_anom_scores)\n",
    "        subj_trues = torch.vstack((S_nom_trues.cpu(), S_anom_trues.cpu()))\n",
    "        S_scores = torch.vstack((S_nom_scores.cpu(), S_anom_scores.cpu()))\n",
    "        \n",
    "        V_nom_scores  = V_scores[0]\n",
    "        V_nom_trues   = torch.ones_like(V_nom_scores)\n",
    "        V_anom_scores = V_scores[1]\n",
    "        V_anom_trues  = torch.zeros_like(V_anom_scores)\n",
    "        verb_trues = torch.vstack((V_nom_trues.cpu(), V_anom_trues.cpu()))\n",
    "        V_scores = torch.vstack((V_nom_scores.cpu(), V_anom_scores.cpu()))\n",
    "        \n",
    "        O_nom_scores  = O_scores[0]\n",
    "        O_nom_trues   = torch.ones_like(O_nom_scores)\n",
    "        O_anom_scores = O_scores[1]\n",
    "        O_anom_trues  = torch.zeros_like(O_anom_scores)\n",
    "        obj_trues = torch.vstack((O_nom_trues.cpu(), O_anom_trues.cpu()))\n",
    "        O_scores = torch.vstack((O_nom_scores.cpu(), O_anom_scores.cpu())) \n",
    "        \n",
    "        subj_partial_auc, verb_partial_auc, obj_partial_auc = aucs\n",
    "\n",
    "        #subj_trues  = torch.flatten(torch.stack(S_y)).data.cpu().numpy()\n",
    "        subj_scores = torch.squeeze(S_scores).detach().numpy()\n",
    "        subj_auc, subj_cov, subj_ci = \\\n",
    "            compute_auc(torch.squeeze(subj_trues), subj_scores, ci_confidence=ci_confidence)\n",
    "\n",
    "        #verb_trues  = torch.flatten(torch.stack(V_y_holdout)).data.cpu().numpy()\n",
    "        verb_scores = torch.squeeze(V_scores).detach().numpy()\n",
    "        verb_auc, verb_cov, verb_ci = \\\n",
    "            compute_auc(torch.squeeze(verb_trues), verb_scores, ci_confidence=ci_confidence)\n",
    "\n",
    "        #obj_trues  = torch.flatten(torch.stack(O_y_holdout)).data.cpu().numpy()\n",
    "        obj_scores = torch.squeeze(O_scores).detach().numpy()\n",
    "        obj_auc, obj_cov, obj_ci = \\\n",
    "            compute_auc(torch.squeeze(obj_trues), obj_scores, ci_confidence=ci_confidence)\n",
    "        \n",
    "        meta_subj_trues.append(subj_trues)\n",
    "        meta_verb_trues.append(verb_trues)\n",
    "        meta_obj_trues.append(obj_trues)\n",
    "        \n",
    "        meta_subj_scores.append(subj_scores)\n",
    "        meta_verb_scores.append(verb_scores)\n",
    "        meta_obj_scores.append(obj_scores)\n",
    "        \n",
    "        subj_partial_aucs.append(subj_partial_auc)\n",
    "        verb_partial_aucs.append(verb_partial_auc)\n",
    "        object_partial_aucs.append(obj_partial_auc)\n",
    "\n",
    "        subj_aucs.append(subj_auc)\n",
    "        verb_aucs.append(verb_auc)\n",
    "        object_aucs.append(obj_auc)\n",
    "        \n",
    "        subj_cis.append(subj_ci)\n",
    "        verb_cis.append(verb_ci)\n",
    "        obj_cis.append(obj_ci)\n",
    "\n",
    "        # Need to compute partial and full AUC for each S,V,O\n",
    "        print(f'\\nRound {j} Complete!')\n",
    "        print(f'Subject Partial AUC is {subj_partial_auc}')\n",
    "        print(f'This is {subj_partial_auc-uns_partial_subject_auc} better than unsupervised')\n",
    "        print(f'Verb Partial AUC is {verb_partial_auc}')\n",
    "        print(f'This is {verb_partial_auc-uns_partial_verb_auc} better than unsupervised')\n",
    "        print(f'Object Partial AUC is {obj_partial_auc}')\n",
    "        print(f'This is {obj_partial_auc-uns_partial_object_auc} better than unsupervised')\n",
    "        print(f'Subject AUC is {subj_auc}')\n",
    "        print(f'This is {subj_auc-uns_subject_auc} better than unsupervised')\n",
    "        print(f'Verb AUC is {verb_auc}')\n",
    "        print(f'This is {verb_auc-uns_verb_auc} better than unsupervised')\n",
    "        print(f'Object AUC is {obj_auc}')\n",
    "        print(f'This is {obj_auc-uns_object_auc} better than unsupervised')\n",
    "\n",
    "    meta_partial_subj_aucs.append(subj_partial_aucs)\n",
    "    meta_partial_verb_aucs.append(verb_partial_aucs)\n",
    "    meta_partial_obj_aucs.append(object_partial_aucs)\n",
    "    meta_subj_aucs.append(subj_aucs)\n",
    "    meta_verb_aucs.append(verb_aucs)\n",
    "    meta_obj_aucs.append(object_aucs)\n",
    "    \n",
    "    meta_meta_subj_trues.append(meta_subj_trues)\n",
    "    meta_meta_verb_trues.append(meta_verb_trues)\n",
    "    meta_meta_obj_trues.append(meta_obj_trues)\n",
    "    meta_meta_subj_scores.append(meta_subj_scores)\n",
    "    meta_meta_verb_scores.append(meta_verb_scores)\n",
    "    meta_meta_obj_scores.append(meta_obj_scores)\n",
    "    \n",
    "    assert(len(meta_meta_subj_trues) != 0)\n",
    "        \n",
    "    # First row of plots:  partial AUROCs\n",
    "    # Second row of plots: complete AUROCS\n",
    "    #%matplotlib notebook\n",
    "    fig_ps, ax_ps = plt.subplots()\n",
    "    fig_pv, ax_pv = plt.subplots()\n",
    "    fig_po, ax_po = plt.subplots()\n",
    "    fig_fs, ax_fs = plt.subplots()\n",
    "    fig_fv, ax_fv = plt.subplots()\n",
    "    fig_fo, ax_fo = plt.subplots()\n",
    "\n",
    "    x = [k for k in range(num_rounds)]\n",
    "    uns_partial_subj = uns_partial_subject_auc * np.ones(len(subj_partial_aucs))\n",
    "    uns_partial_verb = uns_partial_verb_auc * np.ones(len(verb_partial_aucs))\n",
    "    uns_partial_obj  = uns_partial_object_auc * np.ones(len(object_partial_aucs))\n",
    "    uns_subj = uns_subject_auc * np.ones(len(subj_aucs))\n",
    "    uns_verb = uns_verb_auc * np.ones(len(verb_aucs))\n",
    "    uns_obj  = uns_object_auc * np.ones(len(object_aucs))\n",
    "\n",
    "    subj_cl = np.array(subj_cis)[:,0]\n",
    "    subj_cu = np.array(subj_cis)[:,1]\n",
    "\n",
    "    verb_cl = np.array(verb_cis)[:,0]\n",
    "    verb_cu = np.array(verb_cis)[:,1]\n",
    "\n",
    "    obj_cl = np.array(obj_cis)[:,0]\n",
    "    obj_cu = np.array(obj_cis)[:,1]\n",
    "\n",
    "    # Look at AUC confidence here to determine how best to plot it\n",
    "    # then make appropriate changes in the block below to save the plots\n",
    "\n",
    "    ax_ps.plot(x, uns_partial_subj, label='unsupervised')\n",
    "    ax_ps.plot(x, subj_partial_aucs, label='supervised')\n",
    "    ax_ps.set_xlabel('Rounds')\n",
    "    ax_ps.set_ylabel('AUROC')\n",
    "    ax_ps.set_title(f'Subject Partial AUC Split {i}')\n",
    "    ax_ps.legend()\n",
    "\n",
    "    fig_ps.savefig(f'learning_curves/cv_ps{i}.png', format='png')\n",
    "\n",
    "    ax_pv.plot(x, uns_partial_verb, label='unsupervised')\n",
    "    ax_pv.plot(x, verb_partial_aucs, label='supervised')\n",
    "    ax_pv.set_xlabel('Rounds')\n",
    "    ax_pv.set_ylabel('AUROC')\n",
    "    ax_pv.set_title(f'Verb Partial AUC Split {i}')\n",
    "    ax_pv.legend()\n",
    "\n",
    "    fig_pv.savefig(f'learning_curves/cv_pv{i}.png', format='png')\n",
    "\n",
    "    ax_po.plot(x, uns_partial_obj, label='unsupervised')\n",
    "    ax_po.plot(x, object_partial_aucs, label='supervised')\n",
    "    ax_po.set_xlabel('Rounds')\n",
    "    ax_po.set_ylabel('AUROC')\n",
    "    ax_po.set_title(f'Object Partial AUC Split {i}')\n",
    "    ax_po.legend()\n",
    "\n",
    "    fig_po.savefig(f'learning_curves/cv_po{i}.png', format='png')\n",
    "\n",
    "    ax_fs.plot(x, uns_subj, label='unsupervised')\n",
    "    ax_fs.plot(x, subj_aucs, label='supervised')\n",
    "    ax_fs.plot(x, subj_cl, linestyle='dashed',label=f'{int(ci_confidence*100)}% confidence lower bound')\n",
    "    ax_fs.plot(x, subj_cu, linestyle='dashed',label=f'{int(ci_confidence*100)}% confidence upper bound')\n",
    "    ax_fs.set_xlabel('Rounds')\n",
    "    ax_fs.set_ylabel('AUROC')\n",
    "    ax_fs.set_title(f'Subject AUC Split {i}')\n",
    "    ax_fs.legend()\n",
    "\n",
    "    fig_fs.savefig(f'learning_curves/cv_fs{i}.png', format='png')\n",
    "\n",
    "    ax_fv.plot(x, uns_verb, label='unsupervised')\n",
    "    ax_fv.plot(x, verb_aucs, label='supervised')\n",
    "    ax_fv.plot(x, verb_cl, linestyle='dashed',label=f'{int(ci_confidence*100)}% confidence lower bound')\n",
    "    ax_fv.plot(x, verb_cu, linestyle='dashed',label=f'{int(ci_confidence*100)}% confidence upper bound')\n",
    "    ax_fv.set_xlabel('Rounds')\n",
    "    ax_fv.set_ylabel('AUROC')\n",
    "    ax_fv.set_title(f'Verb AUC Split {i}')\n",
    "    ax_fv.legend()\n",
    "\n",
    "    fig_fv.savefig(f'learning_curves/cv_fv{i}.png', format='png')\n",
    "\n",
    "    ax_fo.plot(x, uns_obj, label='unsupervised')\n",
    "    ax_fo.plot(x, object_aucs, label='supervised')\n",
    "    ax_fo.plot(x, obj_cl, linestyle='dashed', label=f'{int(ci_confidence*100)}% confidence lower bound')\n",
    "    ax_fo.plot(x, obj_cu, linestyle='dashed',label=f'{int(ci_confidence*100)}% confidence upper bound')\n",
    "    ax_fo.set_xlabel('Rounds')\n",
    "    ax_fo.set_ylabel('AUROC')\n",
    "    ax_fo.set_title(f'Object AUC Split {i}')\n",
    "    ax_fo.legend()\n",
    "\n",
    "    fig_fo.savefig(f'learning_curves/cv_fo{i}.png', format='png')\n",
    "\n",
    "# Compute Confidence Intervals over full AUCs\n",
    "s_true_flat = np.array(meta_meta_subj_trues).swapaxes(0,1)\n",
    "s_true_flat = s_true_flat.reshape(s_true_flat.shape[0],-1)\n",
    "v_true_flat = np.array(meta_meta_verb_trues).swapaxes(0,1)\n",
    "v_true_flat = v_true_flat.reshape(v_true_flat.shape[0],-1)\n",
    "o_true_flat = np.array(meta_meta_obj_trues).swapaxes(0,1)\n",
    "o_true_flat = o_true_flat.reshape(o_true_flat.shape[0],-1)\n",
    "\n",
    "s_score_flat = np.array(meta_meta_subj_scores).swapaxes(0,1)\n",
    "s_score_flat = s_score_flat.reshape(s_score_flat.shape[0],-1)\n",
    "v_score_flat = np.array(meta_meta_verb_trues).swapaxes(0,1)\n",
    "v_score_flat = v_score_flat.reshape(v_score_flat.shape[0],-1)\n",
    "o_score_flat = np.array(meta_meta_obj_scores).swapaxes(0,1)\n",
    "o_score_flat = o_score_flat.reshape(o_score_flat.shape[0],-1)\n",
    "\n",
    "agg_s_auc = []\n",
    "agg_s_ci  = []\n",
    "agg_v_auc = []\n",
    "agg_v_ci  = []\n",
    "agg_o_auc = []\n",
    "agg_o_ci  = []\n",
    "\n",
    "agg_sp_auc = []\n",
    "agg_vp_auc = []\n",
    "agg_op_auc = []\n",
    "\n",
    "round_count = len(s_true_flat)\n",
    "\n",
    "for i in range(round_count):\n",
    "    sti = np.array(s_true_flat[i])\n",
    "    ssi = np.array(s_score_flat[i])\n",
    "    vti = np.array(v_true_flat[i])\n",
    "    vsi = np.array(v_score_flat[i])\n",
    "    oti = np.array(o_true_flat[i])\n",
    "    osi = np.array(o_score_flat[i])\n",
    "    s_auc, s_cov, s_ci = \\\n",
    "                compute_auc(sti, ssi, ci_confidence=ci_confidence)\n",
    "    v_auc, v_cov, v_ci = \\\n",
    "                compute_auc(vti, vsi, ci_confidence=ci_confidence)\n",
    "    o_auc, o_cov, o_ci = \\\n",
    "                compute_auc(oti, osi, ci_confidence=ci_confidence)\n",
    "    \n",
    "    sp_auc = roc_auc_score(sti, ssi, max_fpr = 0.25)\n",
    "    vp_auc = roc_auc_score(vti, vsi, max_fpr = 0.25)\n",
    "    op_auc = roc_auc_score(oti, osi, max_fpr = 0.25)\n",
    "    \n",
    "    agg_s_auc.append(s_auc)\n",
    "    agg_v_auc.append(v_auc)\n",
    "    agg_o_auc.append(o_auc)\n",
    "    agg_s_ci.append(s_ci)\n",
    "    agg_v_ci.append(v_ci)\n",
    "    agg_o_ci.append(o_ci)\n",
    "    \n",
    "    agg_sp_auc.append(sp_auc)\n",
    "    agg_vp_auc.append(vp_auc)\n",
    "    agg_op_auc.append(op_auc)\n",
    "\n",
    "fig_agg_ps, ax_agg_ps = plt.subplots()\n",
    "fig_agg_pv, ax_agg_pv = plt.subplots()\n",
    "fig_agg_po, ax_agg_po = plt.subplots()\n",
    "fig_agg_fs, ax_agg_fs = plt.subplots()\n",
    "fig_agg_fv, ax_agg_fv = plt.subplots()\n",
    "fig_agg_fo, ax_agg_fo = plt.subplots()\n",
    "\n",
    "x = [k for k in range(round_count)]\n",
    "uns_partial_subj = uns_partial_subject_auc * np.ones(len(subj_partial_aucs))\n",
    "uns_partial_verb = uns_partial_verb_auc * np.ones(len(verb_partial_aucs))\n",
    "uns_partial_obj  = uns_partial_object_auc * np.ones(len(object_partial_aucs))\n",
    "uns_subj = uns_subject_auc * np.ones(round_count)\n",
    "uns_verb = uns_verb_auc * np.ones(round_count)\n",
    "uns_obj  = uns_object_auc * np.ones(round_count)\n",
    "\n",
    "agg_subj_cl = np.array(agg_s_ci)[:,0]\n",
    "agg_subj_cu = np.array(agg_s_ci)[:,1]\n",
    "\n",
    "agg_verb_cl = np.array(agg_v_ci)[:,0]\n",
    "agg_verb_cu = np.array(agg_v_ci)[:,1]\n",
    "\n",
    "agg_obj_cl = np.array(agg_o_ci)[:,0]\n",
    "agg_obj_cu = np.array(agg_o_ci)[:,1]\n",
    "\n",
    "\n",
    "# Look at AUC confidence here to determine how best to plot it\n",
    "# then make appropriate changes in the block below to save the plots\n",
    "\n",
    "ax_agg_ps.plot(x, uns_partial_subj, label='unsupervised')\n",
    "ax_agg_ps.plot(x, agg_sp_auc, label='supervised')\n",
    "ax_agg_ps.set_xlabel('Rounds')\n",
    "ax_agg_ps.set_ylabel('AUROC')\n",
    "ax_agg_ps.set_title(f'Aggregated Subject Partial AUROC')\n",
    "ax_agg_ps.legend()\n",
    "\n",
    "fig_agg_ps.savefig(f'learning_curves/cv_agg_ps.png', format='png')\n",
    "\n",
    "ax_agg_pv.plot(x, uns_partial_verb, label='unsupervised')\n",
    "ax_agg_pv.plot(x, agg_vp_auc, label='supervised')\n",
    "ax_agg_pv.set_xlabel('Rounds')\n",
    "ax_agg_pv.set_ylabel('AUROC')\n",
    "ax_agg_pv.set_title(f'Aggregated Verb Partial AUROC')\n",
    "ax_agg_pv.legend()\n",
    "\n",
    "fig_agg_pv.savefig(f'learning_curves/cv_agg_pv.png', format='png')\n",
    "\n",
    "ax_agg_po.plot(x, uns_partial_obj, label='unsupervised')\n",
    "ax_agg_po.plot(x, agg_op_auc, label='supervised')\n",
    "ax_agg_po.set_xlabel('Rounds')\n",
    "ax_agg_po.set_ylabel('AUROC')\n",
    "ax_agg_po.set_title(f'Aggregated Object Partial AUROC')\n",
    "ax_agg_po.legend()\n",
    "\n",
    "fig_agg_po.savefig(f'learning_curves/cv_agg_po.png', format='png')\n",
    "\n",
    "ax_agg_fs.plot(x, uns_subj, label='unsupervised')\n",
    "ax_agg_fs.plot(x, agg_s_auc, label='supervised')\n",
    "ax_agg_fs.plot(x, agg_subj_cl, linestyle='dashed',label=f'{int(ci_confidence*100)}% confidence lower bound')\n",
    "ax_agg_fs.plot(x, agg_subj_cu, linestyle='dashed',label=f'{int(ci_confidence*100)}% confidence upper bound')\n",
    "ax_agg_fs.set_xlabel('Rounds')\n",
    "ax_agg_fs.set_ylabel('AUROC')\n",
    "ax_agg_fs.set_title(f'Aggregated Subject AUROC')\n",
    "ax_agg_fs.legend()\n",
    "\n",
    "fig_agg_fs.savefig(f'learning_curves/cv_agg_fs.png', format='png')\n",
    "\n",
    "ax_agg_fv.plot(x, uns_verb, label='unsupervised')\n",
    "ax_agg_fv.plot(x, agg_v_auc, label='supervised')\n",
    "ax_agg_fv.plot(x, agg_verb_cl, linestyle='dashed',label=f'{int(ci_confidence*100)}% confidence lower bound')\n",
    "ax_agg_fv.plot(x, agg_verb_cu, linestyle='dashed',label=f'{int(ci_confidence*100)}% confidence upper bound')\n",
    "ax_agg_fv.set_xlabel('Rounds')\n",
    "ax_agg_fv.set_ylabel('AUROC')\n",
    "ax_agg_fv.set_title(f'Aggregated Verb AUROC')\n",
    "ax_agg_fv.legend()\n",
    "\n",
    "fig_agg_fv.savefig(f'learning_curves/cv_agg_fv.png', format='png')\n",
    "\n",
    "ax_agg_fo.plot(x, uns_obj, label='unsupervised')\n",
    "ax_agg_fo.plot(x, agg_o_auc, label='supervised')\n",
    "ax_agg_fo.plot(x, agg_obj_cl, linestyle='dashed', label=f'{int(ci_confidence*100)}% confidence lower bound')\n",
    "ax_agg_fo.plot(x, agg_obj_cu, linestyle='dashed',label=f'{int(ci_confidence*100)}% confidence upper bound')\n",
    "ax_agg_fo.set_xlabel('Rounds')\n",
    "ax_agg_fo.set_ylabel('AUROC')\n",
    "ax_agg_fo.set_title(f'Aggregated Object AUROC')\n",
    "ax_agg_fo.legend()\n",
    "\n",
    "fig_agg_fo.savefig(f'learning_curves/cv_agg_fo.png', format='png')    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00c7479",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_verb_cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3530be56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Confidence Intervals over full AUCs\n",
    "s_true_flat = np.array(meta_meta_subj_trues).swapaxes(0,1)\n",
    "s_true_flat = s_true_flat.reshape(s_true_flat.shape[0],-1)\n",
    "v_true_flat = np.array(meta_meta_verb_trues).swapaxes(0,1)\n",
    "v_true_flat = v_true_flat.reshape(v_true_flat.shape[0],-1)\n",
    "o_true_flat = np.array(meta_meta_obj_trues).swapaxes(0,1)\n",
    "o_true_flat = o_true_flat.reshape(o_true_flat.shape[0],-1)\n",
    "\n",
    "s_score_flat = np.array(meta_meta_subj_scores).swapaxes(0,1)\n",
    "s_score_flat = s_score_flat.reshape(s_score_flat.shape[0],-1)\n",
    "v_score_flat = np.array(meta_meta_verb_trues).swapaxes(0,1)\n",
    "v_score_flat = v_score_flat.reshape(v_score_flat.shape[0],-1)\n",
    "o_score_flat = np.array(meta_meta_obj_scores).swapaxes(0,1)\n",
    "o_score_flat = o_score_flat.reshape(o_score_flat.shape[0],-1)\n",
    "\n",
    "agg_s_auc = []\n",
    "agg_s_ci  = []\n",
    "agg_v_auc = []\n",
    "agg_v_ci  = []\n",
    "agg_o_auc = []\n",
    "agg_o_ci  = []\n",
    "\n",
    "agg_sp_auc = []\n",
    "agg_vp_auc = []\n",
    "agg_op_auc = []\n",
    "\n",
    "round_count = len(s_true_flat)\n",
    "\n",
    "for i in range(round_count):\n",
    "    sti = np.array(s_true_flat[i])\n",
    "    ssi = np.array(s_score_flat[i])\n",
    "    vti = np.array(v_true_flat[i])\n",
    "    vsi = np.array(v_score_flat[i])\n",
    "    oti = np.array(o_true_flat[i])\n",
    "    osi = np.array(o_score_flat[i])\n",
    "    s_auc, s_cov, s_ci = \\\n",
    "                compute_auc(sti, ssi, ci_confidence=ci_confidence)\n",
    "    v_auc, v_cov, v_ci = \\\n",
    "                compute_auc(vti, vsi, ci_confidence=ci_confidence)\n",
    "    o_auc, o_cov, o_ci = \\\n",
    "                compute_auc(oti, osi, ci_confidence=ci_confidence)\n",
    "    \n",
    "    sp_auc = roc_auc_score(sti, ssi, max_fpr = 0.25)\n",
    "    vp_auc = roc_auc_score(vti, vsi, max_fpr = 0.25)\n",
    "    op_auc = roc_auc_score(oti, osi, max_fpr = 0.25)\n",
    "    \n",
    "    agg_s_auc.append(s_auc)\n",
    "    agg_v_auc.append(v_auc)\n",
    "    agg_o_auc.append(o_auc)\n",
    "    agg_s_ci.append(s_ci)\n",
    "    agg_v_ci.append(v_ci)\n",
    "    agg_o_ci.append(o_ci)\n",
    "    \n",
    "    agg_sp_auc.append(sp_auc)\n",
    "    agg_vp_auc.append(vp_auc)\n",
    "    agg_op_auc.append(op_auc)\n",
    "\n",
    "fig_agg_ps, ax_agg_ps = plt.subplots()\n",
    "fig_agg_pv, ax_agg_pv = plt.subplots()\n",
    "fig_agg_po, ax_agg_po = plt.subplots()\n",
    "fig_agg_fs, ax_agg_fs = plt.subplots()\n",
    "fig_agg_fv, ax_agg_fv = plt.subplots()\n",
    "fig_agg_fo, ax_agg_fo = plt.subplots()\n",
    "\n",
    "x = [k for k in range(round_count)]\n",
    "uns_partial_subj = uns_partial_subject_auc * np.ones(len(subj_partial_aucs))\n",
    "uns_partial_verb = uns_partial_verb_auc * np.ones(len(verb_partial_aucs))\n",
    "uns_partial_obj  = uns_partial_object_auc * np.ones(len(object_partial_aucs))\n",
    "uns_subj = uns_subject_auc * np.ones(round_count)\n",
    "uns_verb = uns_verb_auc * np.ones(round_count)\n",
    "uns_obj  = uns_object_auc * np.ones(round_count)\n",
    "\n",
    "agg_subj_cl = np.array(agg_s_ci)[:,0]\n",
    "agg_subj_cu = np.array(agg_s_ci)[:,1]\n",
    "\n",
    "agg_verb_cl = np.array(agg_v_ci)[:,0]\n",
    "agg_verb_cu = np.array(agg_v_ci)[:,1]\n",
    "\n",
    "agg_obj_cl = np.array(agg_o_ci)[:,0]\n",
    "agg_obj_cu = np.array(agg_o_ci)[:,1]\n",
    "\n",
    "\n",
    "# Look at AUC confidence here to determine how best to plot it\n",
    "# then make appropriate changes in the block below to save the plots\n",
    "\n",
    "ax_agg_ps.plot(x, uns_partial_subj, label='unsupervised')\n",
    "ax_agg_ps.plot(x, agg_sp_auc, label='supervised')\n",
    "ax_agg_ps.set_xlabel('Rounds')\n",
    "ax_agg_ps.set_ylabel('AUROC')\n",
    "ax_agg_ps.set_title(f'Aggregated Subject Partial AUROC')\n",
    "ax_agg_ps.legend()\n",
    "\n",
    "fig_agg_ps.savefig(f'learning_curves/agg_ps.png', format='png')\n",
    "\n",
    "ax_agg_pv.plot(x, uns_partial_verb, label='unsupervised')\n",
    "ax_agg_pv.plot(x, agg_vp_auc, label='supervised')\n",
    "ax_agg_pv.set_xlabel('Rounds')\n",
    "ax_agg_pv.set_ylabel('AUROC')\n",
    "ax_agg_pv.set_title(f'Aggregated Verb Partial AUROC')\n",
    "ax_agg_pv.legend()\n",
    "\n",
    "fig_agg_pv.savefig(f'learning_curves/agg_pv.png', format='png')\n",
    "\n",
    "ax_agg_po.plot(x, uns_partial_obj, label='unsupervised')\n",
    "ax_agg_po.plot(x, agg_op_auc, label='supervised')\n",
    "ax_agg_po.set_xlabel('Rounds')\n",
    "ax_agg_po.set_ylabel('AUROC')\n",
    "ax_agg_po.set_title(f'Aggregated Object Partial AUROC')\n",
    "ax_agg_po.legend()\n",
    "\n",
    "fig_agg_po.savefig(f'learning_curves/agg_po.png', format='png')\n",
    "\n",
    "ax_agg_fs.plot(x, uns_subj, label='unsupervised')\n",
    "ax_agg_fs.plot(x, agg_s_auc, label='supervised')\n",
    "ax_agg_fs.plot(x, agg_subj_cl, linestyle='dashed',label=f'{int(ci_confidence*100)}% confidence lower bound')\n",
    "ax_agg_fs.plot(x, agg_subj_cu, linestyle='dashed',label=f'{int(ci_confidence*100)}% confidence upper bound')\n",
    "ax_agg_fs.set_xlabel('Rounds')\n",
    "ax_agg_fs.set_ylabel('AUROC')\n",
    "ax_agg_fs.set_title(f'Aggregated Subject AUROC')\n",
    "ax_agg_fs.legend()\n",
    "\n",
    "fig_agg_fs.savefig(f'learning_curves/agg_fs.png', format='png')\n",
    "\n",
    "ax_agg_fv.plot(x, uns_verb, label='unsupervised')\n",
    "ax_agg_fv.plot(x, agg_v_auc, label='supervised')\n",
    "ax_agg_fv.plot(x, agg_verb_cl, linestyle='dashed',label=f'{int(ci_confidence*100)}% confidence lower bound')\n",
    "ax_agg_fv.plot(x, agg_verb_cu, linestyle='dashed',label=f'{int(ci_confidence*100)}% confidence upper bound')\n",
    "ax_agg_fv.set_xlabel('Rounds')\n",
    "ax_agg_fv.set_ylabel('AUROC')\n",
    "ax_agg_fv.set_title(f'Aggregated Verb AUROC')\n",
    "ax_agg_fv.legend()\n",
    "\n",
    "fig_agg_fv.savefig(f'learning_curves/agg_fv.png', format='png')\n",
    "\n",
    "ax_agg_fo.plot(x, uns_obj, label='unsupervised')\n",
    "ax_agg_fo.plot(x, agg_o_auc, label='supervised')\n",
    "ax_agg_fo.plot(x, agg_obj_cl, linestyle='dashed', label=f'{int(ci_confidence*100)}% confidence lower bound')\n",
    "ax_agg_fo.plot(x, agg_obj_cu, linestyle='dashed',label=f'{int(ci_confidence*100)}% confidence upper bound')\n",
    "ax_agg_fo.set_xlabel('Rounds')\n",
    "ax_agg_fo.set_ylabel('AUROC')\n",
    "ax_agg_fo.set_title(f'Aggregated Object AUROC')\n",
    "ax_agg_fo.legend()\n",
    "\n",
    "fig_agg_fo.savefig(f'learning_curves/agg_fo.png', format='png')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10f2a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "mylist = [[[1,1,1,1,1,1],[2,2,1,1,1,1],[3,5,1,1,1,1]],[[1,1,1,1,1,1],[2,2,1,1,1,1],[3,5,1,1,1,1]],[[1,1,1,1,1,1],[2,2,1,1,1,1],[3,5,1,1,1,1]],[[1,1,1,1,1,1],[2,2,1,1,1,1],[3,5,1,1,1,1]]]\n",
    "myarray = np.array(mylist)\n",
    "# dim rounds: 4, dim trials: 3, dim examples: 6\n",
    "# NEED TO COMBINE FIRST AND LAST DIMENSIONS SOMEHOW\n",
    "myarray = myarray.swapaxes(0,1)\n",
    "myarray = myarray.reshape(myarray.shape[0],-1)\n",
    "print(myarray)\n",
    "\n",
    "#myflat = myarray.reshape(3,-1)\n",
    "#print(myflat)\n",
    "#np.average(myarray,axis=0)\n",
    "print(myarray.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c4d6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3967c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c635b800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First row of plots:  partial AUROCs\n",
    "# Second row of plots: complete AUROCS\n",
    "#%matplotlib notebook\n",
    "fig_ps, ax_ps = plt.subplots()\n",
    "fig_pv, ax_pv = plt.subplots()\n",
    "fig_po, ax_po = plt.subplots()\n",
    "fig_fs, ax_fs = plt.subplots()\n",
    "fig_fv, ax_fv = plt.subplots()\n",
    "fig_fo, ax_fo = plt.subplots()\n",
    "\n",
    "x = [k for k in range(num_rounds)]\n",
    "uns_partial_subj = uns_partial_subject_auc * np.ones(len(subj_partial_aucs))\n",
    "uns_partial_verb = uns_partial_verb_auc * np.ones(len(verb_partial_aucs))\n",
    "uns_partial_obj  = uns_partial_object_auc * np.ones(len(object_partial_aucs))\n",
    "uns_subj = uns_subject_auc * np.ones(len(subj_aucs))\n",
    "uns_verb = uns_verb_auc * np.ones(len(verb_aucs))\n",
    "uns_obj  = uns_object_auc * np.ones(len(object_aucs))\n",
    "\n",
    "subj_cl = np.array(subj_cis)[:,0]\n",
    "subj_cu = np.array(subj_cis)[:,1]\n",
    "\n",
    "verb_cl = np.array(verb_cis)[:,0]\n",
    "verb_cu = np.array(verb_cis)[:,1]\n",
    "\n",
    "obj_cl = np.array(obj_cis)[:,0]\n",
    "obj_cu = np.array(obj_cis)[:,1]\n",
    "\n",
    "\n",
    "# Look at AUC confidence here to determine how best to plot it\n",
    "# then make appropriate changes in the block below to save the plots\n",
    "\n",
    "ax_ps.plot(x, uns_partial_subj, label='unsupervised')\n",
    "ax_ps.plot(x, subj_partial_aucs, label='supervised')\n",
    "ax_ps.set_xlabel('Rounds')\n",
    "ax_ps.set_ylabel('AUROC')\n",
    "ax_ps.set_title(f'Subject Partial AUC Split {i}')\n",
    "ax_ps.legend()\n",
    "\n",
    "fig_ps.savefig(f'learning_curves/ps{i}.png', format='png')\n",
    "\n",
    "ax_pv.plot(x, uns_partial_verb, label='unsupervised')\n",
    "ax_pv.plot(x, verb_partial_aucs, label='supervised')\n",
    "ax_pv.set_xlabel('Rounds')\n",
    "ax_pv.set_ylabel('AUROC')\n",
    "ax_pv.set_title(f'Verb Partial AUC Split {i}')\n",
    "ax_pv.legend()\n",
    "\n",
    "fig_pv.savefig(f'learning_curves/pv{i}.png', format='png')\n",
    "\n",
    "ax_po.plot(x, uns_partial_obj, label='unsupervised')\n",
    "ax_po.plot(x, object_partial_aucs, label='supervised')\n",
    "ax_po.set_xlabel('Rounds')\n",
    "ax_po.set_ylabel('AUROC')\n",
    "ax_po.set_title(f'Object Partial AUC Split {i}')\n",
    "ax_po.legend()\n",
    "\n",
    "fig_po.savefig(f'learning_curves/po{i}.png', format='png')\n",
    "\n",
    "ax_fs.plot(x, uns_subj, label='unsupervised')\n",
    "ax_fs.plot(x, subj_aucs, label='supervised')\n",
    "ax_fs.plot(x, subj_cl, linestyle='dashed',label='95% confidence lower bound')\n",
    "ax_fs.plot(x, subj_cu, linestyle='dashed',label='95% confidence upper bound')\n",
    "ax_fs.set_xlabel('Rounds')\n",
    "ax_fs.set_ylabel('AUROC')\n",
    "ax_fs.set_title(f'Subject AUC Split {i}')\n",
    "ax_fs.legend()\n",
    "\n",
    "fig_fs.savefig(f'learning_curves/fs{i}.png', format='png')\n",
    "\n",
    "ax_fv.plot(x, uns_verb, label='unsupervised')\n",
    "ax_fv.plot(x, verb_aucs, label='supervised')\n",
    "ax_fv.plot(x, verb_cl, linestyle='dashed',label='95% confidence lower bound')\n",
    "ax_fv.plot(x, verb_cu, linestyle='dashed',label='95% confidence upper bound')\n",
    "ax_fv.set_xlabel('Rounds')\n",
    "ax_fv.set_ylabel('AUROC')\n",
    "ax_fv.set_title(f'Verb AUC Split {i}')\n",
    "ax_fv.legend()\n",
    "\n",
    "fig_fv.savefig(f'learning_curves/fv{i}.png', format='png')\n",
    "\n",
    "ax_fo.plot(x, uns_obj, label='unsupervised')\n",
    "ax_fo.plot(x, object_aucs, label='supervised')\n",
    "ax_fo.plot(x, obj_cl, linestyle='dashed', label='95% confidence lower bound')\n",
    "ax_fo.plot(x, obj_cu, linestyle='dashed',label='95% confidence upper bound')\n",
    "ax_fo.set_xlabel('Rounds')\n",
    "ax_fo.set_ylabel('AUROC')\n",
    "ax_fo.set_title(f'Object AUC Split {i}')\n",
    "ax_fo.legend()\n",
    "\n",
    "fig_fo.savefig(f'learning_curves/fo{i}.png', format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb3f36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute Centered Difference Plots to\n",
    "# get an idea of the rate of change along the way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d177548b",
   "metadata": {},
   "source": [
    "This Section plots bar charts showing the class prevalence\n",
    "of the training and holdout sets. Place after line 203 (indicated in the code block above) to activate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebec060",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ntrain_s_anoms = torch.histc(S_y_train.float())[0]\n",
    "ntrain_s_noms  = torch.histc(S_y_train.float())[-1]\n",
    "\n",
    "ntrain_v_anoms = torch.histc(V_y_train.float())[0]\n",
    "ntrain_v_noms  = torch.histc(V_y_train.float())[-1]\n",
    "\n",
    "ntrain_o_anoms = torch.histc(O_y_train.float())[0]\n",
    "ntrain_o_noms  = torch.histc(O_y_train.float())[-1]\n",
    "\n",
    "n_s_anoms = torch.histc(S_y.float())[0]\n",
    "n_s_noms  = torch.histc(S_y.float())[-1]\n",
    "\n",
    "n_v_anoms = torch.histc(V_y.float())[0]\n",
    "n_v_noms  = torch.histc(V_y.float())[-1]\n",
    "\n",
    "n_o_anoms = torch.histc(O_y.float())[0]\n",
    "n_o_noms  = torch.histc(O_y.float())[-1]\n",
    "\n",
    "nholdout_s_anoms = n_s_anoms - ntrain_s_anoms\n",
    "nholdout_v_anoms = n_v_anoms - ntrain_v_anoms\n",
    "nholdout_o_anoms = n_o_anoms - ntrain_o_anoms\n",
    "\n",
    "nholdout_s_noms  = n_s_noms  - ntrain_s_noms\n",
    "nholdout_v_noms  = n_v_noms  - ntrain_v_noms\n",
    "nholdout_o_noms  = n_o_noms  - ntrain_o_noms\n",
    "\n",
    "train_anom_data = [ntrain_s_anoms, ntrain_v_anoms, ntrain_o_anoms]\n",
    "train_nom_data  = [ntrain_s_noms, ntrain_v_noms, ntrain_o_noms]\n",
    "\n",
    "holdout_anom_data = [nholdout_s_anoms, nholdout_v_anoms, nholdout_o_anoms]\n",
    "holdout_nom_data  = [nholdout_s_noms, nholdout_v_noms, nholdout_o_noms]\n",
    "\n",
    "width = 0.3\n",
    "x_ticks_labels = ['S', 'V', 'O']\n",
    "train_fig, train_ax = plt.subplots()\n",
    "holdout_fig, holdout_ax = plt.subplots()\n",
    "train_ax.bar(np.arange(3), train_anom_data, width=width, label='anomalies')\n",
    "train_ax.bar(np.arange(3)+width, train_nom_data, width=width, label='nominals')\n",
    "train_ax.set_xticks(np.arange(3))\n",
    "train_ax.set_xticklabels(x_ticks_labels, fontsize=18)\n",
    "train_ax.set_ylabel('Number of Instances')\n",
    "train_ax.set_title(f'Feedback Pool (Training Data) Split {i}')\n",
    "train_ax.legend()\n",
    "train_fig.savefig(f'learning_curves/train_histogram{i}.png', format='png')\n",
    "\n",
    "holdout_ax.bar(np.arange(3), holdout_anom_data, width=width, label='anomalies')\n",
    "holdout_ax.bar(np.arange(3)+width, holdout_nom_data, width=width, label='nominals')\n",
    "holdout_ax.set_xticks(np.arange(3))\n",
    "holdout_ax.set_xticklabels(x_ticks_labels, fontsize=18)\n",
    "holdout_ax.set_ylabel('Number of Instances')\n",
    "holdout_ax.set_title(f'Validation Data Split {i}')\n",
    "holdout_ax.legend()\n",
    "holdout_fig.savefig(f'learning_curves/holdout_histogram{i}.png', format='png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80498a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average over all splits (rows) for all 6 auc types\n",
    "# Then generate 6 average plots.\n",
    "avg_ps_aucs = np.average(np.array(meta_partial_subj_aucs),axis=0)\n",
    "avg_pv_aucs = np.average(np.array(meta_partial_verb_aucs),axis=0)\n",
    "avg_po_aucs = np.average(np.array(meta_partial_obj_aucs),axis=0)\n",
    "avg_s_aucs = np.average(np.array(meta_subj_aucs),axis=0)\n",
    "avg_v_aucs = np.average(np.array(meta_verb_aucs),axis=0)\n",
    "avg_o_aucs = np.average(np.array(meta_obj_aucs),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c8784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_ps.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef69b12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_pv.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a1e544",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_po.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bf6c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_fs.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ac333e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_fv.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58115825",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c8ec49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
